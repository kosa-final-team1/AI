import streamlit as st
from st_chat_message import message
from typing import TypedDict
from deep_translator import GoogleTranslator
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver
import torch
from transformers import AutoProcessor, AutoModelForZeroShotImageClassification
from dotenv import load_dotenv
import numpy as np
import os
from langgraph.errors import GraphRecursionError
from langchain_core.runnables import RunnableConfig
import chromadb

local_path = "/Users/choibyounghoon/Desktop/test/"
chroma_db = "chromadb_j_0816"

# Chroma í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = chromadb.PersistentClient(path=local_path + chroma_db)

# ì €ì¥ëœ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ
image_collection = client.get_collection("outfit_img_embeddings")
text_collection = client.get_collection("outfit_txt_embeddings")
products_collection = client.get_collection("products_metadatas")

# ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
collections = client.list_collections()

# ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
for collection in collections:
    print(f"Collection name: {collection.name}")
    print("-" * 40)

# GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Fashion-CLIP ëª¨ë¸ ì„¤ì •
processor = AutoProcessor.from_pretrained("patrickjohncyh/fashion-clip")
model = AutoModelForZeroShotImageClassification.from_pretrained(
    "patrickjohncyh/fashion-clip"
).to(device)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
openai_api_key = os.getenv("OPENAI_API_KEY")

# Streamlit ì œëª© ì„¤ì •
st.title("ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” íŒ¨ì…˜ ì±—ë´‡ ğŸ’¬")

# GraphState ì •ì˜
class GraphState(TypedDict):
    session_id: str
    user_input_text: str
    translated_text: str
    user_input_image: str
    user_intent: str
    input_type: str
    retrieved_metadata: dict
    chatbot_answer: str
    chatbot_intent: str
    location: str
    context: str
    season: str
    preferred_style: str
    user_age: int
    user_gender: str


# ì…ë ¥ ìœ í˜• ì²´í¬
def typeCheck(state: GraphState):
    if state["user_input_text"] is None:
        state["input_type"] = "ì´ë¯¸ì§€"
    else:
        state["input_type"] = "í…ìŠ¤íŠ¸"
    return state


# ì˜ë„ ë¶„ë¥˜ ë° ë²ˆì—­
def trans_user_input(state: GraphState):
    user_input = state["user_input_text"]
    # save_user_input_history(state["session_id"], user_input)

    translator = GoogleTranslator(source="auto", target="en")
    translated_text = translator.translate(user_input)
    state["translated_text"] = translated_text
    return state


def intent_classification(state: GraphState):
    intent_llm = ChatOpenAI(model="gpt-4o", temperature=0)
    template = """
    ë‹¹ì‹ ì€ íŒ¨ì…˜ ì–´ì‹œìŠ¤í„´íŠ¸ ì±—ë´‡ì…ë‹ˆë‹¤.
    {user_input} ì— ëŒ€í•´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.
    ë˜í•œ êµ¬ë§¤ ì˜ë„ëŠ” ì‚¬ìš©ìì˜ ì´ì „ ì˜ë„ ê¸°ë¡ì´ ì¶”ì²œì´ì—ˆì„ ê²½ìš°ì—ë§Œ ë¶„ë¥˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.


    # ì‚¬ìš©ì ì˜ë„ì˜ ì¢…ë¥˜
    [ì¶”ì²œ, êµ¬ë§¤, ì¼ë°˜]

    #ì¶œë ¥ í˜•ì‹ì€ ë‹¨ìˆœíˆ ì˜ë„ë§Œ ì¶œë ¥
    """

    #session_history = get_session_history(state["session_id"])
    #intent_history = get_chat_intent_history(state["session_id"])

    prompt = ChatPromptTemplate.from_template(template)
    intent_chain = prompt | intent_llm
    state["user_intent"] = intent_chain.invoke(
        {
            "user_input": state["user_input_text"],
        }
    ).content

    return state
    

# ì¶”ì²œ ì‹œìŠ¤í…œ
def recommend(state: GraphState):
    translator = GoogleTranslator(source="ko", target="en")
    translated_text = translator.translate(state["user_input_text"])
    state["translated_text"] = translated_text

    inputs = processor(
        text=[translated_text], return_tensors="pt", padding=True, truncation=True
    )
    with torch.no_grad():
        text_embeddings = model.get_text_features(**inputs).cpu().numpy()

    text_embeddings = text_embeddings / np.linalg.norm(
        text_embeddings, ord=2, axis=-1, keepdims=True
    )

    # ì˜ˆì œì—ì„œëŠ” text_collection ì´ë¼ëŠ” ì™¸ë¶€ ì»¬ë ‰ì…˜ì—ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•œë‹¤ê³  ê°€ì •
    results = image_collection.query(
        query_embeddings=text_embeddings.tolist(),
        n_results=1,
        include=["metadatas", "distances"],
    )

    a = results["metadatas"][0]
    b = a[0]
    c = b['outfit_img_url']
    state["retrieved_metadata"] = c
    
    return state


# ì¼ë°˜ì ì¸ ì‘ë‹µ ì²˜ë¦¬
def general(state: GraphState):
    return state


# ChatPromptTemplateì„ ì‚¬ìš©í•œ ì±—ë´‡ ì‘ë‹µ ìƒì„±
def chatbot_output(state: GraphState):
    output_llm = ChatOpenAI(model="gpt-4o", temperature=0)

    # ChatPromptTemplateìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì •ì˜
    template = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ì‚¬ìš©ìì˜ ì…ë ¥ì— ëŒ€í•´ì„œ ì‚¬ìš©ì ì˜ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì¡°ì–¸í•´ì£¼ëŠ” Assistant ì±—ë´‡ì…ë‹ˆë‹¤.",
            ),
            (
                "system",
                "ì‚¬ìš©ìì˜ ì…ë ¥ê³¼ ì‚¬ìš©ì ì´ì „ ì…ë ¥ ê·¸ë¦¬ê³  ì±—ë´‡ì˜ ì´ì „ ì‘ë‹µì„ ì°¸ê³ í•˜ì—¬ ê¼¬ë¦¬ ì§ˆë¬¸ì„ í•˜ê³ , ì´ë¥¼ í†µí•´ ì‚¬ìš©ìì˜ í˜„ì¬ ìƒí™©ì„ ì•Œì•„ëƒ…ë‹ˆë‹¤.",
            ),
            ("system", "ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ êµ¬ë§¤ë¥¼ ì´ëŒì–´ë‚´ì„¸ìš”."),
            (
                "system",
                "ì´ë•Œ ê¼¬ë¦¬ ì§ˆë¬¸ì€ í•œ ë²ˆì— í•˜ë‚˜ì”© í•˜ë„ë¡ í•˜ê³  ë©€í‹°í„´ ëŒ€í™”ë¥¼ í†µí•´ ì •ë³´ë¥¼ íšë“í•˜ì„¸ìš”.",
            ),
            ("system", "5ë²ˆì˜ ë©€í‹°í„´ ëŒ€í™” ì•ˆì— ì¶”ì²œì„ ì§„í–‰í•˜ì„¸ìš”."),
            (
                "system",
                "ì•Œì•„ë‚´ë©´ ì¢‹ì€ ì‚¬ìš©ìì˜ ìƒí™©:\n- ì–´ë””ì— ê°€ì„œ ì…ì„ ì˜·ì¸ê°€ìš”?\n- ì–´ë–¤ ìƒí™©ì—ì„œ ì…ì„ ì˜·ì¸ê°€ìš”?\n- í˜„ì¬ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?",
            ),
            ("user", "ì‚¬ìš©ì ì…ë ¥: {user_input}"),
            ("assistant", "ê²€ìƒ‰ëœ ë©”íƒ€ë°ì´í„°: {retrieved_metadata}"),
        ]
    )

    # st.session_state ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    #user_input_history = get_user_input_history(state["session_id"])
    #session_history = get_session_history(state["session_id"])
    #intent_history = get_chat_intent_history(state["session_id"])

    # ChatPromptTemplateì„ ì´ìš©í•œ ì²´ì¸ ìƒì„± ë° ì‹¤í–‰
    chatbot_chain = template | output_llm

    state["chatbot_answer"] = chatbot_chain.invoke(
        {
            "user_input": state["user_input_text"],
            #"user_input_history": user_input_history,
            #"session_history": session_history,
            #"intent_history": intent_history,
            "retrieved_metadata": state["retrieved_metadata"],
        }
    ).content

    # ì„¸ì…˜ ê¸°ë¡ì— ì €ì¥
    #save_to_session_history(state["session_id"], state["chatbot_answer"])

    return state


# ì±—ë´‡ ì‘ë‹µì˜ ì˜ë„ íŒŒì•…
def output_check(state: GraphState):
    output_intent_llm = ChatOpenAI(model="gpt-4o")
    template = """
    ë‹¹ì‹ ì€ ì±—ë´‡ì˜ ë‹µë³€ì´ ì–´ë–¤ ëª©ì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ ì•Œì•„ë‚´ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
    {chatbot_answer} ì— ëŒ€í•´ í•´ë‹¹ ë‹µë³€ì˜ ì˜ë„ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.

    #ì±—ë´‡ ë‹µë³€ ì˜ë„ì˜ ì¢…ë¥˜
    [ì¶”ì²œ ë‹µë³€, êµ¬ë§¤ ë‹µë³€, ê¼¬ë¦¬ ì§ˆë¬¸]

    #ì¶œë ¥ í˜•ì‹ì€ ë‹¨ìˆœíˆ ì˜ë„ë§Œ ì¶œë ¥
    """
    prompt = ChatPromptTemplate.from_template(template)
    intent_chain = prompt | output_intent_llm
    state["chatbot_intent"] = intent_chain.invoke(
        {"chatbot_answer": state["chatbot_answer"]}
    ).content
    return state


# ë¼ìš°í„° í•¨ìˆ˜ë“¤
def router1(state: GraphState):
    return state["input_type"]


def router2(state: GraphState):
    return state["user_intent"]


def router3(state: GraphState):
    return state["chatbot_intent"]


# StateGraph ì´ˆê¸°í™”
workflow = StateGraph(GraphState)

workflow.add_node(typeCheck)
workflow.add_node(intent_classification)
workflow.add_node(recommend)
workflow.add_node(general)
workflow.add_node(chatbot_output)
workflow.add_node(output_check)

# ë…¸ë“œ ê°„ì˜ ì—°ê²° ì„¤ì •
workflow.add_conditional_edges(
    "typeCheck",
    router1,
    {
        "í…ìŠ¤íŠ¸": "intent_classification",
        "ì´ë¯¸ì§€": "recommend",
    },
)

workflow.add_conditional_edges(
    "intent_classification",
    router2,
    {
        "ì¶”ì²œ": "recommend",
        "êµ¬ë§¤": "chatbot_output",
        "ì¼ë°˜": "general",
    },
)

workflow.add_edge("recommend", "chatbot_output")
workflow.add_edge("general", "chatbot_output")
workflow.add_edge("chatbot_output", "output_check")

# ê·¸ë˜í”„ì˜ ì‹œì‘ì  ë° ëì  ì„¤ì •
workflow.set_entry_point("typeCheck")
workflow.set_finish_point("output_check")

# ê¸°ë¡ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ ì„¤ì •
memory = MemorySaver()

# ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•˜ì—¬ ì•± ì‹œì‘
app = workflow.compile(checkpointer=memory)

with st.sidebar:
    session_id = st.text_input("ì„¸ì…˜ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”:", "default_session")
    user_input_image = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”:")


user_input_text = st.text_input("í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”:")

if st.button("ì „ì†¡"):
    state = GraphState(
        session_id=session_id,
        user_input_text=user_input_text,
        translated_text=None,
        user_input_image=None,
        user_intent=None,
        input_type=None,
        retrieved_metadata={},
        chatbot_answer=None,
        chatbot_intent=None,
        location=None,
        context=None,
        season=None,
        preferred_style=None,
        user_age=None,
        user_gender=None,
    )

    if user_input_image:
        state["user_input_image"] = user_input_image

    if user_input_text:
        state = trans_user_input(state)

    
    config = RunnableConfig(recursion_limit=100, configurable={"thread_id": "chatbot"})

    state = app.invoke(state, config=config)

        # User's input displayed in chat format
    message(user_input_text, is_user=True)  # ì‚¬ìš©ìì˜ ì…ë ¥ì„ í‘œì‹œ

    # Chatbot's response displayed in chat format
    message(state["chatbot_answer"], is_user=False)  # ì±—ë´‡ì˜ ì‘ë‹µì„ í‘œì‹œ

    # If there is an image to display
    if "retrieved_metadata" in state and state["retrieved_metadata"]:
        st.image(state["retrieved_metadata"], caption="ì¶”ì²œ ì´ë¯¸ì§€")