import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_teddynote.models import MultiModal

from dotenv import load_dotenv
import os

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("[Error] OpenAI API Keyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤..")
        
# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

st.set_page_config(page_title="AI Fashion Chatbot", page_icon="ðŸ‘š")
st.title(":gray[_AI FashionBot_]_Set the Looks ðŸ‘šðŸ”Ž")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages"] = []

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")

    # ì´ë¯¸ì§€ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"], accept_multiple_files=False)
    
    # Process ë²„íŠ¼ ëˆŒë €ì„ ë•Œ ì´ë²¤íŠ¸ ì •ì˜
    process = st.button("Process")
    
    # ëª¨ë¸ ì„ íƒ ë©”ë‰´
    selected_model = st.selectbox("LLM ì„ íƒ", ["gpt-4o", "gpt-4o-mini"], index=0)

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    system_prompt = st.text_area(
        "system prompt",
        """
        You are an intelligent and stylish fashion recommendation chatbot designed to assist users in discovering fashion outfits. Your goal is to recommend relevant clothing items based on the user's input, which could be text descriptions, images, or a combination of both. You should also retrieve and display visually similar items along with related information when applicable.

        ### Instructions:

        1. **Input Handling**:
            - You will receive different types of inputs from the user: text descriptions of fashion preferences, images of clothing items, or both.
            - If the user provides **only text** (e.g., describing a style, occasion, or type of clothing), generate outfit recommendations based on the description. Consider the context, fashion trends, and style preferences inferred from the user's input.
            - If the user provides **only an image** of a clothing item, use visual similarity analysis to find and recommend up to three fashion items that closely match the given image. Retrieve and display additional information such as brand, material, or styling suggestions.
            - If the user provides **both text and an image**, combine the information from both modalities to generate the most relevant fashion recommendations. Prioritize synergy between the described style and the visual features of the provided image.

        2. **RAG (Retrieval-Augmented Generation)**:
            - After receiving the userâ€™s input, conduct a multimodal RAG process to search a database of fashion items.
            - Retrieve the top 3 most similar clothing items based on the input (textual or visual).
            - For each recommended item, retrieve relevant information, including but not limited to:
                - Brand and material
                - Price range
                - Available colors and sizes
                - Related styling tips
                - Where to buy the item (if applicable)
            - Ensure that the information is accurate and properly formatted for a seamless user experience.

        3. **Presentation of Results**:
            - Display the recommended items in a clear, visually appealing manner. For each recommendation, include:
                - A high-quality image of the clothing item
                - Key details such as item name, brand, material, and price range
                - Additional context such as style tips and related outfit suggestions
            - Ensure that the user experience is smooth and visually engaging, whether they are interacting via text or image.

        4. **Fashion Expertise**:
            - Maintain a tone of voice that is friendly, knowledgeable, and trendy.
            - Stay up-to-date with current fashion trends, seasonal styles, and iconic outfits.
            - Offer personalized fashion advice and styling tips that align with the userâ€™s preferences.

        5. **Example Scenarios**:
            - **Text Input Example**: The user asks, "I'm going to a summer beach party, and I want a casual yet chic outfit." In this case, recommend outfits such as light dresses, sandals, and accessories that are suitable for a summer beach party.
            - **Image Input Example**: The user provides an image of a red leather jacket. Recommend up to three similar jackets, along with suggestions for how to style them (e.g., pairing with jeans, boots, etc.).
            - **Text and Image Input Example**: The user sends an image of a floral dress and says, "I want something similar but more appropriate for work." Recommend similar dresses that are work-appropriate, highlighting the differences in style and formality.

        6. **Edge Cases**:
            - If the user's input is unclear or the provided image is difficult to interpret, ask follow-up questions to clarify their preferences.
            - If no similar items are found, suggest alternative items that closely match the input and explain why they were chosen.

        7. **User Experience Focus**:
            - Ensure that your responses are clear and concise while remaining highly informative.
            - Make the experience engaging by incorporating visuals and details in a way that feels interactive and personalized.
            - Prioritize helping the user make confident and stylish decisions by offering well-rounded fashion advice.
        """,
        height=500,
    )

# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# ì´ë¯¸ì§€ì„ ìºì‹œ ì €ìž¥(ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦¬ëŠ” ìž‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)
@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬ ì¤‘ìž…ë‹ˆë‹¤...")
def process_imagefile(file):
    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ìž¥í•©ë‹ˆë‹¤.
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"

    with open(file_path, "wb") as f:
        f.write(file_content)

    return file_path


# í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
def generate_text_answer(system_prompt, user_prompt, model_name="gpt-4o"):
    # ê°ì²´ ìƒì„±
    llm = ChatOpenAI(
        temperature=0,
        model_name=model_name,  # ëª¨ë¸ëª…
    )

    # GPT-4 ëª¨ë¸ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ì‘ë‹µ ìƒì„±
    messages = [
        ChatMessage(role="system", content=system_prompt),
        ChatMessage(role="user", content=user_prompt)
    ]
    response = llm(messages)
    
    return response.content  # AIMessage ê°ì²´ì˜ content ì†ì„±ì„ ë°˜í™˜


# ì²´ì¸ ìƒì„± (ì´ë¯¸ì§€ ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ ìƒì„±)
def generate_multimodal_answer(image_filepath, system_prompt, user_prompt, model_name="gpt-4o"):
    # ê°ì²´ ìƒì„±
    llm = ChatOpenAI(
        temperature=0,
        model_name=model_name,  # ëª¨ë¸ëª…
    )

    # ë©€í‹°ëª¨ë‹¬ ê°ì²´ ìƒì„± (ì˜ˆì‹œë¡œ MultiModal ì‚¬ìš©)
    multimodal = MultiModal(llm, system_prompt=system_prompt, user_prompt=user_prompt)

    # ì´ë¯¸ì§€ íŒŒì¼ë¡œë¶€í„° ì§ˆì˜(ìŠ¤íŠ¸ë¦¼ ë°©ì‹)
    answer = multimodal.stream(image_filepath)
    return answer


# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìžì˜ ìž…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

# ë§Œì•½ì— ì‚¬ìš©ìž ìž…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    # ì‚¬ìš©ìžì˜ ìž…ë ¥ì„ ì¶”ê°€
    st.chat_message("user").write(user_input)

    if uploaded_file:
        # ì´ë¯¸ì§€ê°€ ìžˆìœ¼ë©´ ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì²˜ë¦¬
        image_filepath = process_imagefile(uploaded_file)
        
        # ì´ë¯¸ì§€ ì‹œê°í™” ì¶”ê°€
        with st.chat_message("user"):
            st.image(image_filepath, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€")

        response = generate_multimodal_answer(image_filepath, system_prompt, user_input, selected_model)

        with st.chat_message("assistant"):
            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
            container = st.empty()

            ai_answer = ""
            for token in response:
                ai_answer += token.content
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.
        add_message("user", user_input)
        add_message("assistant", ai_answer)

    else:
        # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
        text_answer = generate_text_answer(system_prompt, user_input, selected_model)

        with st.chat_message("assistant"):
            st.write(text_answer)

        # ëŒ€í™”ê¸°ë¡ì„ ì €ìž¥í•œë‹¤.
        add_message("user", user_input)
        add_message("assistant", text_answer)
        
import openai

# ë²ˆì—­
def translate_text(text, target_lang="en"):
    try:
        # GPT-4 ëª¨ë¸ì„ ì‚¬ìš©í•´ ë²ˆì—­ ìˆ˜í–‰
        prompt = f"Translate the following text to {target_lang}: {text}"
        
        # GPT-4 API í˜¸ì¶œ
        response = openai.Completion.create(
            engine="gpt-4",  # ì‚¬ìš©í•  ëª¨ë¸ (gpt-3.5-turboë„ ì‚¬ìš© ê°€ëŠ¥)
            prompt=prompt,
            max_tokens=200,  # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´
            temperature=0.3  # ì‘ë‹µì˜ ì¼ê´€ì„±ì„ ìœ„í•œ ë‚®ì€ ì˜¨ë„ ì„¤ì •
        )
        
        # GPT-4 ì‘ë‹µì—ì„œ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        translation = response.choices[0].text.strip()
        return translation
    
    except Exception as e:
        print(f"Error during translation: {e}")
        return text  # ë²ˆì—­ì— ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    
    
# í…ìŠ¤íŠ¸ ìž„ë² ë”© í›„ ì •ê·œí™”
def embed_text(text, model):
    #ë²ˆì—­ fn ë¨¼ì € ì‹¤í–‰ë˜ê²Œ ì¶”ê°€
    translated_text = translate_text(text, target_lang="en")

    text_embedding = model.encode_text([translated_text], batch_size=32)
    text_embedding = text_embedding / np.linalg.norm(text_embedding, ord=2, axis=-1, keepdims=True)
    return text_embedding

# ì´ë¯¸ì§€ ìž„ë² ë”© í•¨ìˆ˜ í›„ ì •ê·œí™”
def embed_image(image, model):
    image_embedding = model.encode_images([image], batch_size=32)
    image_embedding = image_embedding / np.linalg.norm(image_embedding, ord=2, axis=-1, keepdims=True)
    return image_embedding

# í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ìž„ë² ë”© í›„ ì •ê·œí™”
def embed_text_and_image(user_text, user_image, fclip_model):
    text_embedding = embed_text(user_text, fclip_model)
    image_embedding = embed_image(user_image, fclip_model)
    return text_embedding, image_embedding
 
# ********************************
# ì‚¬ìš©ìž ì¸í’‹ íƒ€ìž…ì— ë”°ë¥¸ í˜¸ì¶œ
# ********************************

#pip install -U fashion_clip 
from fashion_clip.fashion_clip import FashionCLIP

fclip_model = FashionCLIP('fashion-clip')
if user_input or uploaded_file:
    # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ë‘˜ ë‹¤ ìž…ë ¥ëœ ê²½ìš°
    if user_input and uploaded_file:
        image_filepath = process_imagefile(uploaded_file)
        
        # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = embed_text_and_image(user_input, uploaded_file, fclip_model)
        
        # ì‚¬ìš©ìžì˜ ìž…ë ¥ ì²˜ë¦¬
        st.chat_message("user").write(f"í…ìŠ¤íŠ¸: {user_input}, ì´ë¯¸ì§€ ì—…ë¡œë“œë¨")

        # AIì˜ ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in response:
                ai_answer += token.content
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ ì €ìž¥
        add_message("user", f"í…ìŠ¤íŠ¸: {user_input}, ì´ë¯¸ì§€ ì—…ë¡œë“œë¨")
        add_message("assistant", ai_answer)

    # [ì‚¬ìš©ìž ì¸í’‹ì´ ì´ë¯¸ì§€ì¼ ê²½ìš°]
    elif uploaded_file:
        image_filepath = process_imagefile(uploaded_file)

        # ì´ë¯¸ì§€ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = embed_image(uploaded_file, fclip_model)

        # ì‚¬ìš©ìžì˜ ìž…ë ¥ ì²˜ë¦¬
        st.chat_message("user").write("ì´ë¯¸ì§€ ì—…ë¡œë“œë¨")

        # AIì˜ ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in response:
                ai_answer += token.content
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ ì €ìž¥
        add_message("user", "ì´ë¯¸ì§€ ì—…ë¡œë“œë¨")
        add_message("assistant", ai_answer)

    # [ì‚¬ìš©ìž ì¸í’‹ì´ í…ìŠ¤íŠ¸ì¼ ê²½ìš°]
    elif user_input:
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = embed_text(user_input, fclip_model)

        # ì‚¬ìš©ìžì˜ ìž…ë ¥ ì²˜ë¦¬
        st.chat_message("user").write(user_input)

        # AIì˜ ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in response:
                ai_answer += token.content
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ ì €ìž¥
        add_message("user", user_input)
        add_message("assistant", ai_answer)        
        



        
        
from fashion_clip.fashion_clip import FashionCLIP

fclip_model = FashionCLIP('fashion-clip')
if user_input or uploaded_file:
    # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ë‘˜ ë‹¤ ìž…ë ¥ëœ ê²½ìš°
    if user_input and uploaded_file:
        image_filepath = process_imagefile(uploaded_file)
        
        # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = embed_text_and_image(user_input, uploaded_file, fclip_model)
        
        # ì‚¬ìš©ìžì˜ ìž…ë ¥ ì²˜ë¦¬
        st.chat_message("user").write(f"í…ìŠ¤íŠ¸: {user_input}, ì´ë¯¸ì§€ ì—…ë¡œë“œë¨")

        # AIì˜ ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in response:
                ai_answer += token.content
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ ì €ìž¥
        add_message("user", f"í…ìŠ¤íŠ¸: {user_input}, ì´ë¯¸ì§€ ì—…ë¡œë“œë¨")
        add_message("assistant", ai_answer)

    # [ì‚¬ìš©ìž ì¸í’‹ì´ ì´ë¯¸ì§€ì¼ ê²½ìš°]
    elif uploaded_file:
        image_filepath = process_imagefile(uploaded_file)

        # ì´ë¯¸ì§€ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = embed_image(uploaded_file, fclip_model)

        # ì‚¬ìš©ìžì˜ ìž…ë ¥ ì²˜ë¦¬
        st.chat_message("user").write("ì´ë¯¸ì§€ ì—…ë¡œë“œë¨")

        # AIì˜ ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in response:
                ai_answer += token.content
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ ì €ìž¥
        add_message("user", "ì´ë¯¸ì§€ ì—…ë¡œë“œë¨")
        add_message("assistant", ai_answer)

    # [ì‚¬ìš©ìž ì¸í’‹ì´ í…ìŠ¤íŠ¸ì¼ ê²½ìš°]
    elif user_input:
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = embed_text(user_input, fclip_model)

        # ì‚¬ìš©ìžì˜ ìž…ë ¥ ì²˜ë¦¬
        st.chat_message("user").write(user_input)

        # AIì˜ ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in response:
                ai_answer += token.content
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ ì €ìž¥
        add_message("user", user_input)
        add_message("assistant", ai_answer)        

import chromadb
import os

local_path = '/Users/jei/Downloads/'  
chroma_db = 'chromadb_j_0816'

# Chroma í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = chromadb.PersistentClient(path=local_path+chroma_db)

# ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
collections = client.list_collections()

# ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
for collection in collections:
    print(f"Collection name: {collection.name}")
    print("-" * 40)
    
# ì €ìž¥ëœ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ
img_collection = client.get_collection("outfit_img_embeddings")
txt_collection = client.get_collection("outfit_txt_embeddings")
products_collection = client.get_collection("products_metadatas")

# ë°ì´í„° ìˆ˜ í™•ì¸í•´ë³´ê¸°
print(f"img_collection ë°ì´í„° ìˆ˜ >> : {img_collection.count()}")
print(f"txt_collection  ë°ì´í„° ìˆ˜ >>> : {txt_collection.count()}")
print(f"products_collection  ë°ì´í„° ìˆ˜ >>> : {products_collection.count()}")
print("-" * 50)

# product_amekaji = products_collection.get(
#                     where={
#                     "outfit_category": {
#                         "$eq": "amekaji"
#                      }
#                     }
#                     )